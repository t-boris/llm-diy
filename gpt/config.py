GPT_CONFIG_124M = {
    "vocab_size": 50257,    # Size of the vocabulary.
    "ctx_len": 1024,        # Length of the context window.
    "emd_dim": 768,         # Dimension of the embeddings.
    "n_heads": 12,          # Number of attention heads.
    "n_layers": 12,         # Number of transformer layers.
    "drop_rate": 0.1,       # Dropout rate.
    "qkv_bias": False,      # Whether to include bias in the QKV linear layers.
}